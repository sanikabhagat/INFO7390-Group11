{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "\n",
    "import pandas as pd\n",
    "import urllib as ulib\n",
    "import requests\n",
    "import os\n",
    "import re #Regular Expression\n",
    "from io import BytesIO\n",
    "\n",
    "# To parse the document, BeautifulSoup library is used. \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# To read and open zipfile, zipfile module is used\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "\n",
    "#####S3#######\n",
    "import boto\n",
    "import boto.s3\n",
    "import sys\n",
    "from boto.s3.key import Key\n",
    "##############\n",
    "######chethan#######\n",
    "import datetime\n",
    "import time\n",
    "ts = time.time()\n",
    "global dt\n",
    "global year\n",
    "dt = datetime.datetime.fromtimestamp(ts).strftime('%Y%m%d_%H%M%S')\n",
    "#######year for file??????????\n",
    "#year = '2017'\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Year you want the analysis for\n",
      "Enter year from 2003 to 2017: 2017\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter the Year you want the analysis for\")\n",
    "year = input(\"Enter year from 2003 to 2017: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 21:05:16,928 - DEBUG - Clearing the index array :\n",
      "2018-10-12 21:05:16,930 - DEBUG - []\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "fh = logging.FileHandler('DATACLEAN_'+year+'_'+dt+'.txt')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "global changeindex \n",
    "global nuberofvalidentries\n",
    "global violationcount\n",
    "changeindex = [] ##change\n",
    "logger.debug('Clearing the index array :')\n",
    "logger.debug(changeindex)\n",
    "\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 21:05:16,960 - DEBUG - Starting new HTTPS connection (1): www.sec.gov\n",
      "2018-10-12 21:05:17,068 - DEBUG - https://www.sec.gov:443 \"GET /dera/data/edgar-log-file-data-set.html HTTP/1.1\" 200 11977\n"
     ]
    }
   ],
   "source": [
    "#Now we need to specify the link of the website from where we want to scrape the data\n",
    "page=requests.get(\"https://www.sec.gov/dera/data/edgar-log-file-data-set.html\")\n",
    "\n",
    "# Now that the document has been successfully downloaded, we need to parse the document so that the data needed can be extracted\n",
    "#An instance of the library needs to be created for parsing\n",
    "soup=BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to define a function which will get us all the year links in a list\n",
    "#The div_tag_list will give us the div tags which has all the year links\n",
    "#The a_tag_list will basically store the <a> tags in a list. \n",
    "#But this variable contains a list which has a list of all the a_list_tags. So basically the list has just one element.\n",
    "#So we need to just take that one element as a separate list (named a_tag_list again) and use that list \n",
    "def generate_link_list():\n",
    "    div_tag_list=soup.find_all('div',attrs={'id':'asyncAccordion'})\n",
    "    a_tag_list=[]\n",
    "    for div_tag in div_tag_list:\n",
    "        a_tag=div_tag.find_all('a')\n",
    "        a_tag_list.append(a_tag)\n",
    "    a_tag_list=a_tag_list[0]\n",
    "    year_links=[]\n",
    "    for aa in a_tag_list:\n",
    "        link=aa.get('href')\n",
    "        final_link='https://www.sec.gov'+link\n",
    "        year_links.append(final_link) \n",
    "    return(year_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now that the links are generated, we need a function to check whether the year input by the user is valid or not.\n",
    "# For that we will generate a list which has all the valid years and then will define a function which will check the userinput\n",
    "    \n",
    "def valid_year(user_year_input):\n",
    "    valid_year_list=[]\n",
    "    for i in range(2003,2018):\n",
    "        valid_year_list.append(i)\n",
    "    if user_year_input in valid_year_list:\n",
    "        return('The year is valid')\n",
    "    else:\n",
    "        return('The year is invalid')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we need to define a function which will select the appropriate year link depending upon the user input \n",
    "\n",
    "def user_input_year_link(year):\n",
    "    link_list=generate_link_list()\n",
    "    for year_link in link_list:\n",
    "        if year in year_link:\n",
    "            year=year_link\n",
    "    zip_files = ulib.request.urlopen(year)\n",
    "    soup=BeautifulSoup(zip_files,'html.parser')\n",
    "    a_tag=soup.find_all('a')\n",
    "    a_tag_list=[]\n",
    "    for i in a_tag:\n",
    "        links=i.get('href')\n",
    "        a_tag_list.append(links)\n",
    "    first_day_zip_links=[]\n",
    "    for i in a_tag_list:\n",
    "        if '01.zip' in i:\n",
    "            first_day_zip_links.append(i)\n",
    "    return(first_day_zip_links)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to create a file path on our local where all the zip files will be extracted and csv will be stored\n",
    "\n",
    "def zip_csv_on_local(year):\n",
    "    folder=str(year)\n",
    "    #path= str(os.getcwd())+'/'+folder+dt\n",
    "    path= str(os.getcwd())+'/'+folder\n",
    "    os.makedirs(path)\n",
    "    a=user_input_year_link(year)\n",
    "    for i in a:\n",
    "        with ulib.request.urlopen(i) as firstMonth:\n",
    "            with ZipFile(BytesIO(firstMonth.read())) as unzippedFile:\n",
    "                unzippedFile.extractall(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createfinalfolder(year):\n",
    "    ts = time.time()\n",
    "    dt = datetime.datetime.fromtimestamp(ts).strftime('%Y%m%d_%H%M%S')\n",
    "    folder=str(year)+str(dt)\n",
    "    path= str(os.getcwd())+'/'+folder\n",
    "    os.makedirs(path)\n",
    "    return path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Now we need to load data from each of the csv files that are in the folder to separate dataframes\n",
    "\n",
    "def create_dataframes(i,path):\n",
    "    file=glob.glob(path+'/log*.csv')\n",
    "   # def read_csv(list_val):\n",
    "    #    return pd.read_csv(list_val)\n",
    "    val = file[i]\n",
    "    df=pd.read_csv(val,low_memory=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################chethan##############################\n",
    "#The null report\n",
    "def tofindnull(dt):\n",
    "    logger.info(\"Finding the null values\")\n",
    "    null_columns=dt.columns[dt.isnull().any()]\n",
    "    logger.info(\"The summary of null values : \")\n",
    "    logger.debug(dt[null_columns].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To replace the null values with 0\n",
    "def cleannull(cdt):\n",
    "    logger.info(\"Replacing all the null values with zero\")\n",
    "    cdt.fillna(0, inplace=True)\n",
    "    logger.info(\"NaN values are replaced successfuly\")\n",
    "    logger.info(\"The null report after cleaning\")\n",
    "    tofindnull(cdt)\n",
    "    return cdt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datacorrection(nuberofvalidentries,column,index,dfclean):\n",
    "    logger.info(\"The data correction is being done on \"+str(column))\n",
    "    for i in index:\n",
    "        dfclean.loc[index, column] = 0\n",
    "        logger.debug(\"The value after changing : \")\n",
    "        logger.debug(dfclean.iloc[i][column])\n",
    "    logger.info(\"nuber of valid entries : \"+str(nuberofvalidentries))\n",
    "    logger.info(\"number of non zero values\"+str(dfclean[column].astype(bool).sum(axis=0)))\n",
    "    if(nuberofvalidentries == dfclean[column].astype(bool).sum(axis=0)):\n",
    "        logger.info(\"The dataclean was successful for \"+str(column))\n",
    "        return dfclean\n",
    "    else:\n",
    "        logger.info(\"The dataclean was not successful for \"+str(column))\n",
    "        return dfclean \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check the anamolies at \"browser\" column\n",
    "def browserclean(cdr):  \n",
    "\n",
    "    totalrows = len(cdr)\n",
    "    logger.info(\"The total entries in the dataframe : \"+str(totalrows))\n",
    "    #The expected values\n",
    "    browsers = [\"mie\",\"fox\",\"saf\",\"chr\",\"sea\",\"opr\",\"oth\",\"mac\",\"lin\",\"iph\",\"ipd\",\"and\",\"rim\",\"iem\"]\n",
    "    logger.info(\"The valid browsers name : \")\n",
    "    logger.info(browsers)\n",
    "    logger.info(\"Finding whether browser analysis is possible\")\n",
    "    nuberofvalidentries = 0\n",
    "    violationcount = 0\n",
    "    i=0\n",
    "    for x in cdr[\"browser\"]:\n",
    "          ##change\n",
    "        violation = 1\n",
    "        for y in browsers:\n",
    "            if (x == y):\n",
    "                #logger.info(\"The correct value : \",x)\n",
    "                violation = 0\n",
    "                break\n",
    "        if (violation == 1):\n",
    "            #logger.info(\"the bad value is\",x)\n",
    "            if(x != 0):\n",
    "                changeindex.append(i) ##change\n",
    "            violationcount = violationcount + 1\n",
    "            #x = \"NotAllowed\"\n",
    "        i = i+1  ##change\n",
    "    logger.info(\"The index at which values need to be changed : \")\n",
    "    logger.info(changeindex)\n",
    "    logger.info(\"The number of violations : \"+str(violationcount))\n",
    "    logger.info(\"The total rows : \"+str(totalrows))\n",
    "    nuberofvalidentries = totalrows - violationcount\n",
    "    logger.info(\"Total number of valid entries : \"+str(nuberofvalidentries))\n",
    "    percentofvalidvalues = (nuberofvalidentries/totalrows)*100\n",
    "    if (percentofvalidvalues >= 5):\n",
    "        logger.info(\"The browser analysis is possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 1,nuberofvalidentries\n",
    "    else:\n",
    "        logger.info(\"The browser analysis is not possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 0,nuberofvalidentries\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findclean(cdr):    \n",
    "    totalrows = len(cdr)\n",
    "    logger.info(\"The total entries in the dataframe : \"+str(totalrows))\n",
    "    finds = [\"0.0\",\"1.0\",\"2.0\",\"3.0\",\"4.0\",\"5.0\",\"6.0\",\"7.0\",\"8.0\",\"9.0\",\"10.0\"]\n",
    "    logger.info(\"The valid find values : \")\n",
    "    logger.info(finds)\n",
    "    logger.info(\"Finding whether find analysis is possible\")\n",
    "    nuberofvalidentries = 0\n",
    "    i=0\n",
    "    violationcount = 0\n",
    "    for x in cdr[\"find\"]:\n",
    "        violation = 1\n",
    "        for y in finds:\n",
    "            if (str(x) == y):\n",
    "                #logger.info(\"The correct value : \",x)\n",
    "                violation = 0\n",
    "                break\n",
    "        if (violation == 1):\n",
    "            logger.debug(\"the bad value is\"+str(x))\n",
    "            if(x != 0):\n",
    "                changeindex.append(i) ##change\n",
    "            violationcount = violationcount + 1\n",
    "        i = i+1\n",
    "    logger.info(\"The index at which values need to be changed : \")\n",
    "    logger.debug(changeindex)\n",
    "    logger.info(\"The number of violations : \"+str(violationcount))\n",
    "    logger.info(\"The total rows : \"+str(totalrows))\n",
    "    nuberofvalidentries = totalrows - violationcount\n",
    "    logger.info(\"Total number of valid entries : \"+str(nuberofvalidentries))\n",
    "    percentofvalidvalues = (nuberofvalidentries/totalrows)*100\n",
    "    if (percentofvalidvalues >= 5):\n",
    "        logger.info(\"The find analysis is possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 1,nuberofvalidentries\n",
    "    else:\n",
    "        logger.info(\"The find analysis is not possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 0,nuberofvalidentries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allfloatclean(column,cdr):   \n",
    "    logger.info(\"cleaning the column : \"+str(column))\n",
    "    totalrows = len(cdr)\n",
    "    logger.info(\"The total entries in the dataframe : \"+str(totalrows))\n",
    "    floats = [\"0.0\",\"1.0\"]\n",
    "    logger.info(\"The valid values : \")\n",
    "    logger.info(floats)\n",
    "    logger.info(\"Finding whether analysis is possible\")\n",
    "    nuberofvalidentries = 0\n",
    "    i=0\n",
    "    violationcount = 0\n",
    "    for x in cdr[column]:\n",
    "        violation = 1\n",
    "        for y in floats:\n",
    "            if (str(x) == y):\n",
    "                #logger.info(\"The correct value : \",x)\n",
    "                violation = 0\n",
    "                break\n",
    "        if (violation == 1):\n",
    "            if(x != 0):\n",
    "                changeindex.append(i) ##change\n",
    "            logger.debug(\"the bad value is\"+str(x))\n",
    "            violationcount = violationcount + 1\n",
    "        i = i+1\n",
    "    logger.debug(\"The index at which values need to be changed : \")\n",
    "    logger.debug(changeindex)\n",
    "    logger.info(\"The number of violations : \"+str(violationcount))\n",
    "    logger.info(\"The total rows : \"+str(totalrows))\n",
    "    nuberofvalidentries = totalrows - violationcount\n",
    "    logger.info(\"Total number of valid entries : \"+str(nuberofvalidentries))\n",
    "    percentofvalidvalues = (nuberofvalidentries/totalrows)*100\n",
    "    if (percentofvalidvalues >= 5):\n",
    "        logger.info(\"The analysis is possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 1,nuberofvalidentries\n",
    "    else:\n",
    "        logger.info(\"The analysis is not possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 0,nuberofvalidentries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floattypecheck(column,dfs):\n",
    "    logger.info(\"cleaning the column : \"+str(column))\n",
    "    totalrows = len(dfs)\n",
    "    logger.info(\"The total entries in the dataframe : \"+str(totalrows))\n",
    "    logger.info(\"The valid values : any valid number of type float\")\n",
    "    logger.info(\"Finding whether analysis is possible\")\n",
    "    nuberofvalidentries = 0\n",
    "    i=0\n",
    "    violationcount = 0\n",
    "    for x in dfs[column]:\n",
    "        violation = 1\n",
    "        if(type(x) == float):\n",
    "            #logger.info(\"float\")\n",
    "            violation = 0\n",
    "        else:\n",
    "            logger.info(\"not float\")\n",
    "        if (violation == 1):\n",
    "            logger.debug(\"the bad value is\"+str(x))\n",
    "            if(x != 0):\n",
    "                changeindex.append(i) ##change\n",
    "            violationcount = violationcount + 1\n",
    "        i = i+1\n",
    "    logger.info(\"The index at which values need to be changed : \")\n",
    "    logger.info(changeindex)\n",
    "    logger.info(\"The number of violations : \"+str(violationcount))\n",
    "    logger.info(\"The total rows : \"+str(totalrows))\n",
    "    nuberofvalidentries = totalrows - violationcount\n",
    "    logger.info(\"Total number of valid entries : \"+str(nuberofvalidentries))\n",
    "    percentofvalidvalues = (nuberofvalidentries/totalrows)*100\n",
    "    if (percentofvalidvalues >= 5):\n",
    "        logger.info(\"The analysis is possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 1,nuberofvalidentries\n",
    "    else:\n",
    "        logger.info(\"The analysis is not possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 0,nuberofvalidentries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codecheck(column,dfs):\n",
    "    logger.info(\"cleaning the column : \"+str(column))\n",
    "    totalrows = len(dfs)\n",
    "    logger.info(\"The total entries in the dataframe : \"+str(totalrows))\n",
    "    logger.info(\"The valid values : 1xx 2xx 3xx 4xx 5xx\")\n",
    "    logger.info(\"Finding whether analysis is possible\")\n",
    "    validcodes = [\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "    nuberofvalidentries = 0\n",
    "    i=0\n",
    "    violationcount = 0\n",
    "    for x in dfs[column]:\n",
    "        violation = 1\n",
    "        for y in validcodes:\n",
    "            if (str(int(x/100.0)) == y):\n",
    "                #logger.info(\"The correct value : \",x)\n",
    "                violation = 0\n",
    "                break\n",
    "        if (violation == 1):\n",
    "            #logger.info(\"the bad value is\",x)\n",
    "            violationcount = violationcount + 1\n",
    "            if(x != 0):\n",
    "                changeindex.append(i) ##change\n",
    "        i = i+1\n",
    "    logger.info(\"The index at which values need to be changed : \")\n",
    "    logger.info(changeindex)\n",
    "    logger.info(\"The number of violations : \"+str(violationcount))\n",
    "    logger.info(\"The total rows : \"+str(totalrows))\n",
    "    nuberofvalidentries = totalrows - violationcount\n",
    "    logger.info(\"Total number of valid entries : \"+str(nuberofvalidentries))\n",
    "    percentofvalidvalues = (nuberofvalidentries/totalrows)*100\n",
    "    if (percentofvalidvalues >= 5):\n",
    "        logger.info(\"The analysis is possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 1,nuberofvalidentries\n",
    "    else:\n",
    "        logger.info(\"The analysis is not possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 0,nuberofvalidentries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateclean(column,dfs):\n",
    "    logger.info(\"cleaning the column : \"+str(column))\n",
    "    totalrows = len(dfs)\n",
    "    logger.info(\"The total entries in the dataframe : \"+str(totalrows))\n",
    "    logger.info(\"The valid values : year-month-day\")\n",
    "    logger.info(\"Finding whether analysis is possible\")\n",
    "    nuberofvalidentries = 0\n",
    "    i=0\n",
    "    violationcount = 0\n",
    "    for x in dfs[column]:\n",
    "        violation = 1\n",
    "        inputDate = str(x)\n",
    "        #logger.info(inputDate)\n",
    "        year,day,month = inputDate.split('-')\n",
    "        isValidDate = True\n",
    "        try :\n",
    "            datetime.datetime(int(year),int(month),int(day))\n",
    "        except ValueError :\n",
    "            isValidDate = False\n",
    "        if(isValidDate) :\n",
    "            #logger.info (\"Input date is valid ..\")\n",
    "            violation = 0\n",
    "        if (violation == 1):\n",
    "\n",
    "            logger.debug(\"the bad value is\"+str(x))\n",
    "            if(x != 0):\n",
    "                changeindex.append(i) ##change\n",
    "            violationcount = violationcount + 1\n",
    "        i = i+1\n",
    "    logger.info(\"The index at which values need to be changed : \")\n",
    "    logger.info(changeindex)\n",
    "    logger.info(\"The number of violations : \"+str(violationcount))\n",
    "    logger.info(\"The total rows : \"+str(totalrows))\n",
    "    nuberofvalidentries = totalrows - violationcount\n",
    "    logger.info(\"Total number of valid entries : \"+str(nuberofvalidentries))\n",
    "    percentofvalidvalues = (nuberofvalidentries/totalrows)*100\n",
    "    if (percentofvalidvalues >= 5):\n",
    "        logger.info(\"The analysis is possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 1,nuberofvalidentries\n",
    "    else:\n",
    "        logger.info(\"The analysis is not possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 0,nuberofvalidentries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeclean(column,dfs):\n",
    "    logger.info(\"cleaning the column : \"+str(column))\n",
    "    totalrows = len(dfs)\n",
    "    logger.info(\"The total entries in the dataframe : \"+str(totalrows))\n",
    "    logger.info(\"The valid values : %H:%M:%S\")\n",
    "    logger.info(\"Finding whether analysis is possible\")\n",
    "    nuberofvalidentries = 0\n",
    "    i=0\n",
    "    violationcount = 0\n",
    "    for x in dfs[column]:\n",
    "        violation = 1\n",
    "        timeformat = \"%H:%M:%S\"\n",
    "        try:\n",
    "            validtime = datetime.datetime.strptime(x, timeformat)\n",
    "            #logger.info(\"valid\")\n",
    "            violation = 0\n",
    "        except ValueError:\n",
    "            #Do your logic for invalid format (maybe logger.info some message?).\n",
    "            logger.debug(\" not valid\")\n",
    "        if (violation == 1):\n",
    "            #logger.info(\"the bad value is\",x)\n",
    "            if(x != 0):\n",
    "                changeindex.append(i) ##change\n",
    "            violationcount = violationcount + 1\n",
    "        i = i+1\n",
    "    logger.info(\"The index at which values need to be changed : \")\n",
    "    logger.info(changeindex)\n",
    "    logger.info(\"The number of violations : \"+str(violationcount))\n",
    "    logger.info(\"The total rows : \"+str(totalrows))\n",
    "    nuberofvalidentries = totalrows - violationcount\n",
    "    logger.info(\"Total number of valid entries : \"+str(nuberofvalidentries))\n",
    "    percentofvalidvalues = (nuberofvalidentries/totalrows)*100\n",
    "    if (percentofvalidvalues >= 5):\n",
    "        logger.info(\"The analysis is possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 1,nuberofvalidentries\n",
    "    else:\n",
    "        logger.info(\"The analysis is not possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 0,nuberofvalidentries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipclean(column,dfs):\n",
    "    logger.info(\"cleaning the column : \"+str(column))\n",
    "    totalrows = len(dfs)\n",
    "    logger.info(\"The total entries in the dataframe : \"+str(totalrows))\n",
    "    logger.info(\"The valid values : ###.###.###.xxx\")\n",
    "    logger.info(\"Finding whether analysis is possible\")\n",
    "    nuberofvalidentries = 0\n",
    "    i=0\n",
    "    violationcount = 0\n",
    "    for x in dfs[column]:\n",
    "        violation = 1\n",
    "        ip = str(x)\n",
    "        p1,p2,p3,p4 = x.split('.')\n",
    "        if((type(p1) == str) and (type(p2) == str) and (type(p3) == str) and (type(p4) == str)):\n",
    "            violation = 0\n",
    "        if (violation == 1):\n",
    "            #logger.info(\"the bad value is\",x)\n",
    "            if(x != 0):\n",
    "                changeindex.append(i) ##change\n",
    "            violationcount = violationcount + 1\n",
    "        i = i+1\n",
    "    logger.info(\"The index at which values need to be changed : \")\n",
    "    logger.info(changeindex)\n",
    "    logger.info(\"The number of violations : \"+str(violationcount))\n",
    "    logger.info(\"The total rows : \"+str(totalrows))\n",
    "    nuberofvalidentries = totalrows - violationcount\n",
    "    logger.info(\"Total number of valid entries : \"+str(nuberofvalidentries))\n",
    "    percentofvalidvalues = (nuberofvalidentries/totalrows)*100\n",
    "    if (percentofvalidvalues >= 5):\n",
    "        logger.info(\"The analysis is possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 1,nuberofvalidentries\n",
    "    else:\n",
    "        logger.info(\"The analysis is not possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 0,nuberofvalidentries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tofindnonzero(column,dfnz):\n",
    "    logger.info(\"cleaning the column : \"+str(column))\n",
    "    totalrows = len(dfnz)\n",
    "    logger.info(\"The total entries in the dataframe : \"+str(totalrows))   \n",
    "    logger.info(\"Finding whether analysis is possible\")\n",
    "    violationcount = \"global\"\n",
    "    violationcount = totalrows - dfnz[column].astype(bool).sum(axis=0)\n",
    "    logger.info(\"The number of violations : \"+str(violationcount))\n",
    "    logger.info(\"The total rows : \"+str(totalrows))\n",
    "    nuberofvalidentries = totalrows - violationcount\n",
    "    logger.info(\"Total number of valid entries : \"+str(nuberofvalidentries))\n",
    "    percentofvalidvalues = (nuberofvalidentries/totalrows)*100\n",
    "    if (percentofvalidvalues >= 10):\n",
    "        logger.info(\"The analysis is possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 1\n",
    "    else:\n",
    "        logger.info(\"The analysis is not possible as the value is : \"+str(percentofvalidvalues))\n",
    "        return 0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberrc(dtrc):\n",
    "    logger.info(\"checking whether all columns have same number of rows\")\n",
    "    totalrows = len(dtrc)\n",
    "    logger.info(\"The total entries in the dataframe : \"+str(totalrows))\n",
    "    columns = list(dtrc.head(0))\n",
    "    logger.info(\"The valid columns\"+str(columns))\n",
    "    i = 0\n",
    "    for x in columns:\n",
    "        if (len(dtrc[x]) == totalrows):\n",
    "            logger.info(\"No data missing in : \"+str(x))\n",
    "            i = i + 1\n",
    "        else:\n",
    "            logger.info(\"Number of missing data \"+str(totalrows-len(dtrc[x])))\n",
    "    if(i == len(columns)):\n",
    "        logger.info (\"There are no missing data\")\n",
    "        return 1\n",
    "    else:\n",
    "        logger.info (\"check for missing data\")\n",
    "        return 0      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataclean(df1):\n",
    "    #Null clean is being called\n",
    "    dfc1 = cleannull(df1)\n",
    "    logger.info(\"Null cleaned data\")\n",
    "    #logger.info(dfc1.head(5))\n",
    "    #############################\n",
    "    changeindex = []\n",
    "    logger.info(\"Checking and cleaning the browser\")\n",
    "    resultbrowser,nv = browserclean(dfc1)\n",
    "    logger.info(\"the number of valid values : \"+str(nv))\n",
    "    if (resultbrowser):\n",
    "        logger.info(\"The browser data is sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    else:\n",
    "        logger.error(\"The browser data is not sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        logger.error(\"The browser data is inconsistent for the date : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    #######################################\n",
    "    logger.info(\"index of find where wrong values are present :\")\n",
    "    logger.info(changeindex)\n",
    "    arraysizeforclean = len(changeindex)\n",
    "    if(arraysizeforclean>0):\n",
    "        column = \"browser\"\n",
    "        logger.info(\"cleaning the browser : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        dfc1 = datacorrection(nv,column,changeindex,dfc1)\n",
    "    else:\n",
    "        logger.info(\"Cleaning of data not required as it doesnt contain wrong values : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "\n",
    "\n",
    "    logger.info(\"The (browser)cleaned data \")\n",
    "    logger.info(dfc1.iloc[10][\"date\"])\n",
    "    #dfc1[\"find\"].unique()\n",
    "    ##Browser clean done####\n",
    "    ##duhb##\n",
    "    ######################################################\n",
    "    changeindex = []\n",
    "    logger.info(\"Checking and cleaning the find\")\n",
    "    resultbrowser,nv = findclean(dfc1)\n",
    "    logger.info(\"the number of valid values : \"+str(nv))\n",
    "    if (resultbrowser):\n",
    "        logger.info(\"The browser data is sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    else:\n",
    "        logger.error(\"The browser data is not sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        logger.error(\"The browser data is inconsistent for the date : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    ##############################################\n",
    "    logger.info(\"index of find where wrong values are present :\")\n",
    "    logger.info(changeindex)\n",
    "    arraysizeforclean = len(changeindex)\n",
    "    if(arraysizeforclean>0):\n",
    "        column = \"find\"\n",
    "        logger.info(\"cleaning the browser : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        dfc1 = datacorrection(nv,column,changeindex,dfc1)\n",
    "    else:\n",
    "        logger.info(\"Cleaning of data not required as it doesnt contain wrong values : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "\n",
    "\n",
    "    logger.info(\"The (find)cleaned data \")\n",
    "    logger.info(dfc1.iloc[10][\"date\"])\n",
    "    dfc1[\"find\"].unique()\n",
    "    ##find clean done####\n",
    "    ###duhb###\n",
    "    #############################################\n",
    "    var = [\"crawler\",\"idx\",\"norefer\",\"noagent\"]\n",
    "    for x in var:\n",
    "        logger.info(\"checking on : \"+str(x))\n",
    "        changeindex = []\n",
    "        result= 0\n",
    "        nv = 0\n",
    "        logger.info(\"Checking and cleaning the\"+str(x))\n",
    "        result,nv = allfloatclean(x,dfc1)\n",
    "        logger.info(\"the number of valid values : \"+str(nv))\n",
    "        if (result):\n",
    "            logger.info(\"The \"+str(x)+\" data is sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        else:\n",
    "            logger.error(\"The \"+str(x)+\" data is not sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "            logger.error(\"The \"+str(x)+\" data is inconsistent for the date : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        #calling the cleAN    \n",
    "        logger.info(\"index of find where wrong values are present :\")\n",
    "        logger.info(changeindex)\n",
    "        arraysizeforclean = len(changeindex)\n",
    "        if(arraysizeforclean>0):\n",
    "            logger.info(\"cleaning the\"+str(x)+\":\" +str(dfc1.iloc[10][\"date\"]))\n",
    "            dfc1 = datacorrection(nv,x,changeindex,dfc1)\n",
    "        else:\n",
    "            logger.info(\"Cleaning of data not required as it doesnt contain wrong values : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    ##dufb\n",
    "    #############################\n",
    "    var = [\"crawler\",\"idx\",\"norefer\",\"noagent\"]\n",
    "    for x in var:\n",
    "        logger.info(\"The float values cleaned data \")\n",
    "        logger.info(dfc1.iloc[10][\"date\"])\n",
    "        logger.info(dfc1[x].unique())\n",
    "    ################################\n",
    "    var2 = [\"size\",\"zone\",\"cik\"]\n",
    "    for x in var2:\n",
    "        logger.info(\"checking on : \"+str(x))\n",
    "        changeindex = []\n",
    "        result= 0\n",
    "        nv = 0\n",
    "        logger.info(\"Checking and cleaning the\"+str(x))\n",
    "        result,nv = floattypecheck(x,dfc1)\n",
    "        logger.info(\"the number of valid values : \"+str(nv))\n",
    "        if (result):\n",
    "            logger.info(\"The \"+str(x)+\" data is sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        else:\n",
    "            logger.error(\"The \"+str(x)+\" data is not sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "            logger.error(\"The \"+str(x)+\" data is inconsistent for the date : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        #calling the cleAN    \n",
    "        logger.info(\"index of find where wrong values are present :\")\n",
    "        logger.debug(changeindex)\n",
    "        arraysizeforclean = len(changeindex)\n",
    "        if(arraysizeforclean>0):\n",
    "            logger.info(\"cleaning the\"+str(x)+\":\"+str(dfc1.iloc[10][\"date\"]))\n",
    "            dfc1 = datacorrection(nv,x,changeindex,dfc1)\n",
    "        else:\n",
    "            logger.info(\"Cleaning of data not required as it doesnt contain wrong values : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    ####duhb####\n",
    "    ##############################\n",
    "    var2 = [\"size\",\"zone\",\"cik\"]\n",
    "    for x in var2:\n",
    "        logger.info(\"The float values cleaned data \")\n",
    "        logger.info(dfc1.iloc[10][\"date\"])\n",
    "        cf = dfc1[x].unique()\n",
    "        logger.info(cf.dtype)\n",
    "    ##############################\n",
    "    ###duhb###\n",
    "    changeindex = []\n",
    "    logger.info(\"Checking and cleaning the code\")\n",
    "    column = \"code\"\n",
    "    resultbrowser,nv = codecheck(column,dfc1)\n",
    "    logger.info(\"the number of valid values : \"+str(nv))\n",
    "    if (resultbrowser):\n",
    "        logger.info(\"The browser data is sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    else:\n",
    "        logger.error(\"The browser data is not sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        logger.error(\"The browser data is inconsistent for the date : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    ################################################################################################\n",
    "    logger.info(\"index of find where wrong values are present :\")\n",
    "    logger.info(changeindex)\n",
    "    arraysizeforclean = len(changeindex)\n",
    "    if(arraysizeforclean>0):\n",
    "        column = \"code\"\n",
    "        logger.info(\"cleaning the code : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        dfc1 = datacorrection(nv,column,changeindex,dfc1)\n",
    "    else:\n",
    "        logger.info(\"Cleaning of data not required as it doesnt contain wrong values : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "\n",
    "\n",
    "    logger.info(\"The (code)cleaned data \")\n",
    "    logger.info(dfc1.iloc[10][\"date\"])\n",
    "    dfc1[\"code\"].unique()\n",
    "    ###duhb###\n",
    "    ##code clean done####\n",
    "    ###############################\n",
    "    changeindex = []\n",
    "    logger.info(\"Checking and cleaning the date\")\n",
    "    column = \"date\"\n",
    "    resultbrowser,nv = dateclean(column,dfc1)\n",
    "    logger.info(\"the number of valid values : \"+str(nv))\n",
    "    if (resultbrowser):\n",
    "        logger.info(\"The  data is sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    else:\n",
    "        logger.error(\"The data is not sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        logger.error(\"The data is inconsistent for the date : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    ################################################################################################\n",
    "    logger.info(\"index of find where wrong values are present :\")\n",
    "    logger.info(changeindex)\n",
    "    arraysizeforclean = len(changeindex)\n",
    "    if(arraysizeforclean>0):\n",
    "        column = \"date\"\n",
    "        logger.info(\"cleaning the code : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        dfc1 = datacorrection(nv,column,changeindex,dfc1)\n",
    "    else:\n",
    "        logger.info(\"Cleaning of data not required as it doesnt contain wrong values : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "\n",
    "\n",
    "    logger.info(\"The (date)cleaned data \")\n",
    "    logger.info(dfc1.iloc[10][\"date\"])\n",
    "    logger.debug(dfc1[\"date\"].unique())\n",
    "    ##duhb####\n",
    "    ##date clean done####\n",
    "    ##################################\n",
    "    changeindex = []\n",
    "    logger.info(\"Checking and cleaning the time\")\n",
    "    column = \"time\"\n",
    "    resultbrowser,nv = timeclean(column,dfc1)\n",
    "    logger.info(\"the number of valid values : \"+str(nv))\n",
    "    if (resultbrowser):\n",
    "        logger.info(\"The  data is sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    else:\n",
    "        logger.error(\"The data is not sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        logger.error(\"The data is inconsistent for the date : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    ################################################################################################\n",
    "    logger.info(\"index of find where wrong values are present :\")\n",
    "    logger.info(changeindex)\n",
    "    arraysizeforclean = len(changeindex)\n",
    "    if(arraysizeforclean>0):\n",
    "        column = \"time\"\n",
    "        logger.info(\"cleaning the code : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        dfc1 = datacorrection(nv,column,changeindex,dfc1)\n",
    "    else:\n",
    "        logger.info(\"Cleaning of data not required as it doesnt contain wrong values : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "\n",
    "\n",
    "    logger.info(\"The (time)cleaned data \")\n",
    "    logger.info(dfc1.iloc[10][\"date\"])\n",
    "###duhb###\n",
    "    ##time clean done####\n",
    "    ###################################\n",
    "    changeindex = []\n",
    "    logger.info(\"Checking and cleaning the ip\")\n",
    "    column = \"ip\"\n",
    "    resultbrowser,nv = ipclean(column,dfc1)\n",
    "    logger.info(\"the number of valid values : \"+str(nv))\n",
    "    if (resultbrowser):\n",
    "        logger.info(\"The  data is sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    else:\n",
    "        logger.error(\"The data is not sufficient for analysis : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        logger.error(\"The data is inconsistent for the date : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "    ################################################################################################\n",
    "    logger.info(\"index of find where wrong values are present :\")\n",
    "    logger.debug(changeindex)\n",
    "    arraysizeforclean = len(changeindex)\n",
    "    if(arraysizeforclean>0):\n",
    "        column = \"ip\"\n",
    "        logger.info(\"cleaning the code : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "        dfc1 = datacorrection(nv,column,changeindex,dfc1)\n",
    "    else:\n",
    "        logger.info(\"Cleaning of data not required as it doesnt contain wrong values : \"+str(dfc1.iloc[10][\"date\"]))\n",
    "\n",
    "\n",
    "    logger.info(\"The (ip)cleaned data \")\n",
    "    logger.info(dfc1.iloc[10][\"date\"])\n",
    "######duhb######\n",
    "    ##ip clean done####\n",
    "    ###################\n",
    "    var3 = [\"cik\",\"accession\",\"extention\"]\n",
    "    for x in var3:\n",
    "        logger.info(\"checking on : \"+str(x))\n",
    "        status = 0\n",
    "        status = tofindnonzero(x,dfc1)\n",
    "        if (status):\n",
    "            logger.info(\"analysis possible with : \"+str(x))\n",
    "        else:\n",
    "            logger.error(\"analysis not possible with : \"+str(x))\n",
    "    ###################\n",
    "    final = numberrc(dfc1)\n",
    "    if(final):\n",
    "        logger.info(\"Data clean is complete\")\n",
    "    else:\n",
    "        logger.error(\"Data has empty entries\")\n",
    "        logger.error(\"Check for the missing columns\")\n",
    "    ###################\n",
    "    return dfc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find the stats of request codes in each day\n",
    "def codestats(code,ds):\n",
    "    totalentries = len(ds)\n",
    "    logger.info(\"Total number of entries in \"+str(totalentries))\n",
    "    writetosummary(\"To find the stats of request codes in each day\")\n",
    "    writetosummary(\"Total number of entries in \"+str(totalentries))\n",
    "    counts = ds[\"code\"].value_counts()\n",
    "    logger.info(\"The stats of response codes on : \"+str(ds.iloc[10][\"date\"]))\n",
    "    writetosummary(\"The stats of response codes on : \"+str(ds.iloc[10][\"date\"]))\n",
    "    logger.info(counts)\n",
    "    writetosummary(counts)\n",
    "    logger.info(\"The total number of \"+str(code)+\" : \"+str(counts[code]))\n",
    "    writetosummary(\"The total number of \"+str(code)+\" : \"+str(counts[code]))\n",
    "    logger.info(\"percentage\"+str((counts[code]/totalentries)*100)+\"%\")\n",
    "    writetosummary(\"percentage\"+str((counts[code]/totalentries)*100)+\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeanalysis(ds):\n",
    "    success = ds['size'][ds['code'] == 200.0].sum()\n",
    "    totalsize = ds['size'].sum()\n",
    "    logger.info(\"Total data size accessed on the date : \"+str(ds.iloc[10][\"date\"]))\n",
    "    writetosummary(\"Total data size accessed on the date : \"+str(ds.iloc[10][\"date\"]))\n",
    "    logger.info(totalsize)\n",
    "    writetosummary(str(totalsize))\n",
    "    logger.info(\"successful data size accessed on the date : \"+str(ds.iloc[10][\"date\"]))\n",
    "    writetosummary(\"successful data size accessed on the date : \"+str(ds.iloc[10][\"date\"]))\n",
    "    logger.info(success)\n",
    "    writetosummary(str(success))\n",
    "    logger.info(\"percentage success\"+str((success/totalsize)*100)+\"%\")\n",
    "    writetosummary(\"percentage success\"+str((success/totalsize)*100)+\"%\")\n",
    "    logger.info(\"Average size of the dat being accessed : \"+str(int(ds['size'].mean())))\n",
    "    writetosummary(\"Average size of the dat being accessed : \"+str(int(ds['size'].mean())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topfiveip(ds):\n",
    "    logger.info(\"To find the top 5 IP addresses making maximum number of requets in the day : \"+str(ds.iloc[10]['date']))\n",
    "    writetosummary(\"the top 5 IP addresses making maximum number of requets in the day : \"+str(ds.iloc[10]['date']))\n",
    "    logger.info(\"grouping data by IP\")\n",
    "    #writetosummary()\n",
    "    ipds = ds.groupby('ip', as_index=False).agg({\"cik\": \"count\"})\n",
    "    logger.info(\"sorting in descending order\")\n",
    "    #writetosummary()\n",
    "    ipds=ipds.sort_values(by=['cik'], ascending=False)\n",
    "    logger.info(\"TOP 5 IPs\")\n",
    "    writetosummary(\"TOP 5 IPs\")\n",
    "    ipdsp = ipds.head(5)\n",
    "    logger.info(ipdsp)\n",
    "    writetosummary(str(ipdsp))\n",
    "    logger.info(\"percentage of requests from top 1 IP :\"+str(ipdsp.iloc[0]['ip']))\n",
    "    writetosummary(\"percentage of requests from top 1 IP :\"+str(ipdsp.iloc[0]['ip']))\n",
    "    logger.info(str((ipdsp.iloc[0]['cik']/len(ds))*100))\n",
    "    writetosummary(str((ipdsp.iloc[0]['cik']/len(ds))*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busyhour(tds):\n",
    "    logger.info(\"To find the bussiest hour of the day : \"+str(tds.iloc[10]['date']))\n",
    "    writetosummary(\"the bussiest hour of the day : \"+str(tds.iloc[10]['date']))\n",
    "    tds = tds.groupby('time', as_index=False).agg({\"cik\": \"count\"})\n",
    "    totalrequests = tds['cik'].sum()\n",
    "    i=0\n",
    "    sumt = 0\n",
    "    dicth = {}\n",
    "    j =1\n",
    "    for x in tds['cik']:\n",
    "        sumt = sumt + x\n",
    "        i=i+1\n",
    "        if(i == 3600):\n",
    "            dicth[j] = sumt\n",
    "            j = j+1\n",
    "            i = 0\n",
    "            sumt = 0\n",
    "    logger.info(\"The summary of number of requests at all hours\")\n",
    "    writetosummary(\"The summary of number of requests at all hours\")\n",
    "    logger.info(dicth)\n",
    "    writetosummary(str(dicth))\n",
    "    sum(dicth.values())\n",
    "    maxv = max(dicth.values())\n",
    "    logger.info(\"The bussiest hour : \"+str(list(dicth.keys())[list(dicth.values()).index(maxv)])+\" and Number of requests : \"+str(maxv))\n",
    "    writetosummary(\"The bussiest hour : \"+str(list(dicth.keys())[list(dicth.values()).index(maxv)])+\" and Number of requests : \"+str(maxv))\n",
    "    logger.info(\"percentage of requests : \"+str((maxv/totalrequests)*100)+\"%\")\n",
    "    writetosummary(\"percentage of requests : \"+str((maxv/totalrequests)*100)+\"%\")\n",
    "    logger.info(\"Average number of requests per hour : \"+str(int(sum(dicth.values())/len(dicth.values()))))\n",
    "    writetosummary(\"Average number of requests per hour : \"+str(int(sum(dicth.values())/len(dicth.values()))))\n",
    "    logger.info(\"Average percentage : \"+str(int(((sum(dicth.values())/len(dicth.values()))/totalrequests)*100))+\"%\")\n",
    "    writetosummary(\"Average percentage : \"+str(int(((sum(dicth.values())/len(dicth.values()))/totalrequests)*100))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extentionanalysis(ds):\n",
    "    logger.info(\"To find the top 10 page type requeted in the day : \"+str(ds.iloc[10]['date']))\n",
    "    writetosummary(\"The top 10 page type requeted in the day : \"+str(ds.iloc[10]['date']))\n",
    "    logger.info(\"grouping data by extention\")\n",
    "    #writetosummary(\"grouping data by extention\")\n",
    "    epds = ds.groupby('extention', as_index=False).agg({\"cik\": \"count\"})\n",
    "    logger.info(\"sorting in descending order\")\n",
    "    #writetosummary(\"sorting in descending order\")\n",
    "    epds=epds.sort_values(by=['cik'], ascending=False)\n",
    "    logger.info(\"TOP 10 extentions\")\n",
    "    writetosummary(\"TOP 10 extentions\")\n",
    "    epdsp = epds.head(10)\n",
    "    logger.debug(epdsp)\n",
    "    writetosummary(str(epdsp))\n",
    "    logger.info(\"Number of requests with the extention asked frequesntly:\"+str(epdsp.iloc[0]['extention']))\n",
    "    writetosummary(\"Number of requests with the extention asked frequesntly:\"+str(epdsp.iloc[0]['extention']))\n",
    "    logger.info(\"Percent : \")\n",
    "    writetosummary(\"Percent : \")\n",
    "    logger.info(str((epdsp.iloc[0]['cik']/len(ds))*100))\n",
    "    writetosummary(str((epdsp.iloc[0]['cik']/len(ds))*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summaryanalysis(dfa):\n",
    "    logger.info(\"Starting the summary analysis on the cleaned data\")\n",
    "    logger.info(\"Request Response code analysis\")\n",
    "    writetosummary(\"Request Response code analysis\")\n",
    "    logger.info(\"200 OK percentage : Analysis\")\n",
    "    writetosummary(\"200 OK percentage : Analysis\")\n",
    "    code = 200\n",
    "    codestats(code,dfa)\n",
    "    logger.info(\"404 Not found percentage : Analysis\")\n",
    "    writetosummary(\"404 Not found percentage : Analysis\")\n",
    "    code = 404\n",
    "    codestats(code,dfa)\n",
    "    busyhour(dfa)\n",
    "    topfiveip(dfa)\n",
    "    sizeanalysis(dfa)\n",
    "    extentionanalysis(dfa)\n",
    "    logger.info(\"Summary analysis completed\")\n",
    "    writetosummary(\"Summary analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframes_to_csv(df):\n",
    "    ts = time.time()\n",
    "    dt = datetime.datetime.fromtimestamp(ts).strftime('%Y%m%d_%H%M%S')\n",
    "    #folder=str(dt)\n",
    "    df = pd.DataFrame(df)\n",
    "    #csvfilename = str(year)+\"-01\"\n",
    "    csvfilename=str(dt)\n",
    "    print(path2)\n",
    "    df.to_csv(path2+'/'+csvfilename+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipdir(path, ziph):\n",
    "    path=str(path)\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipdirmain():\n",
    "    zipf = zipfile.ZipFile('Python.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "    zipdir(path2, zipf)\n",
    "    zipf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createfile(year):\n",
    "    global filename\n",
    "    filename = path2+'/'+\"Summary\"+str(year)\n",
    "    f= open(filename,\"a\")\n",
    "    f.write(\"This is the Summary Analysis for the year \"+str(year)+ \"-- First Day of Each Month\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writetosummary(data):\n",
    "    f= open(filename,\"a\")\n",
    "    f.write(str(data)+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 21:05:17,723 - INFO - ######################\n",
      "2018-10-12 21:05:17,724 - INFO - Generating the URL for every year\n",
      "2018-10-12 21:05:17,725 - INFO - ######################\n",
      "2018-10-12 21:05:17,735 - INFO - ######################\n",
      "2018-10-12 21:05:17,736 - INFO - The year is 2017\n",
      "2018-10-12 21:05:17,737 - INFO - ######################\n",
      "2018-10-12 21:05:17,738 - INFO - ######################\n",
      "2018-10-12 21:05:17,740 - INFO - Creating the Folder to store cleaned data\n",
      "2018-10-12 21:05:17,742 - INFO - ######################\n",
      "2018-10-12 21:05:17,743 - INFO - ######################\n",
      "2018-10-12 21:05:17,744 - INFO - Creating file for Summary\n",
      "2018-10-12 21:05:17,745 - INFO - ######################\n",
      "2018-10-12 21:05:17,761 - INFO - ######################\n",
      "2018-10-12 21:05:17,762 - INFO - Downloading the CSV and creating Data Frames for the year entered by the year\n",
      "2018-10-12 21:05:17,763 - INFO - ######################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Desktop\\ADS\\INFO7390-Group11\\Part2/201720181012_210517\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-f0dea4174efc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Downloading the CSV and creating Data Frames for the year entered by the year\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"######################\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mzip_csv_on_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-86eedcf846e6>\u001b[0m in \u001b[0;36mzip_csv_on_local\u001b[1;34m(year)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mulib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfirstMonth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirstMonth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0munzippedFile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                 \u001b[0munzippedFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1554\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m              \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1556\u001b[1;33m             \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1558\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mfdst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "logger.info(\"######################\")\n",
    "logger.info(\"Generating the URL for every year\")\n",
    "logger.info(\"######################\")\n",
    "generate_link_list()\n",
    "\n",
    "logger.info(\"######################\")\n",
    "logger.info(\"The year is \"+str(year))\n",
    "logger.info(\"######################\")\n",
    "\n",
    "logger.info(\"######################\")\n",
    "logger.info(\"Creating the Folder to store cleaned data\")\n",
    "path2=createfinalfolder(year)\n",
    "print(path2)\n",
    "logger.info(\"######################\")\n",
    "\n",
    "logger.info(\"######################\")\n",
    "logger.info(\"Creating file for Summary\")\n",
    "logger.info(\"######################\")\n",
    "createfile(year)\n",
    "\n",
    "logger.info(\"######################\")\n",
    "logger.info(\"Downloading the CSV and creating Data Frames for the year entered by the year\")\n",
    "logger.info(\"######################\")\n",
    "zip_csv_on_local(year)\n",
    "\n",
    "\n",
    "#To read the dataframe\n",
    "logger.info(\"######################\")\n",
    "logger.info(\"Reading the dataframe\")\n",
    "logger.info(\"######################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 21:19:47,300 - INFO - ######################\n",
      "2018-10-12 21:19:47,301 - INFO - The dataframe created successfully\n",
      "2018-10-12 21:19:47,302 - INFO - ######################\n",
      "2018-10-12 21:19:47,303 - INFO - ######################\n",
      "2018-10-12 21:19:47,304 - INFO - printing the dataframes\n",
      "2018-10-12 21:19:47,305 - INFO - ######################\n",
      "2018-10-12 21:19:47,308 - INFO -               ip        date      time  zone        cik             accession  \\\n",
      "0  107.23.85.jfd  2017-02-01  00:00:00   0.0  1013454.0  0001539497-13-000934   \n",
      "1  107.23.85.jfd  2017-02-01  00:00:00   0.0  1407200.0  0001193125-16-541564   \n",
      "2  107.23.85.jfd  2017-02-01  00:00:00   0.0  1013454.0  0001539497-13-000938   \n",
      "3  107.23.85.jfd  2017-02-01  00:00:00   0.0  1013454.0  0001539497-13-000900   \n",
      "4  107.23.85.jfd  2017-02-01  00:00:00   0.0  1407200.0  0001193125-16-535603   \n",
      "\n",
      "    extention   code    size  idx  norefer  noagent  find  crawler  browser  \n",
      "0  -index.htm  200.0  2792.0  1.0      0.0      0.0  10.0      0.0      NaN  \n",
      "1  -index.htm  200.0  2880.0  1.0      0.0      0.0  10.0      0.0      NaN  \n",
      "2  -index.htm  200.0  2792.0  1.0      0.0      0.0  10.0      0.0      NaN  \n",
      "3  -index.htm  200.0  2791.0  1.0      0.0      0.0  10.0      0.0      NaN  \n",
      "4  -index.htm  200.0  2670.0  1.0      0.0      0.0  10.0      0.0      NaN  \n",
      "2018-10-12 21:19:47,334 - INFO - ######################\n",
      "2018-10-12 21:19:47,336 - INFO - Cleaning the data\n",
      "2018-10-12 21:19:47,339 - INFO - ######################\n",
      "2018-10-12 21:19:47,345 - INFO - Replacing all the null values with zero\n",
      "2018-10-12 21:19:48,545 - INFO - NaN values are replaced successfuly\n",
      "2018-10-12 21:19:48,546 - INFO - The null report after cleaning\n",
      "2018-10-12 21:19:48,547 - INFO - Finding the null values\n",
      "2018-10-12 21:19:50,312 - INFO - The summary of null values : \n",
      "2018-10-12 21:19:50,314 - DEBUG - Series([], dtype: float64)\n",
      "2018-10-12 21:19:50,316 - INFO - Null cleaned data\n",
      "2018-10-12 21:19:50,317 - INFO - Checking and cleaning the browser\n",
      "2018-10-12 21:19:50,319 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:19:50,320 - INFO - The valid browsers name : \n",
      "2018-10-12 21:19:50,321 - INFO - ['mie', 'fox', 'saf', 'chr', 'sea', 'opr', 'oth', 'mac', 'lin', 'iph', 'ipd', 'and', 'rim', 'iem']\n",
      "2018-10-12 21:19:50,326 - INFO - Finding whether browser analysis is possible\n",
      "2018-10-12 21:19:54,176 - INFO - The index at which values need to be changed : \n",
      "2018-10-12 21:19:54,177 - INFO - []\n",
      "2018-10-12 21:19:54,178 - INFO - The number of violations : 3252531\n",
      "2018-10-12 21:19:54,179 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:19:54,180 - INFO - Total number of valid entries : 0\n",
      "2018-10-12 21:19:54,181 - INFO - The browser analysis is not possible as the value is : 0.0\n",
      "2018-10-12 21:19:54,182 - INFO - the number of valid values : 0\n",
      "2018-10-12 21:19:54,183 - ERROR - The browser data is not sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:19:54,185 - ERROR - The browser data is inconsistent for the date : 2017-02-01\n",
      "2018-10-12 21:19:54,186 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:19:54,187 - INFO - []\n",
      "2018-10-12 21:19:54,189 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:19:54,190 - INFO - The (browser)cleaned data \n",
      "2018-10-12 21:19:54,191 - INFO - 2017-02-01\n",
      "2018-10-12 21:19:54,192 - INFO - Checking and cleaning the find\n",
      "2018-10-12 21:19:54,193 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:19:54,194 - INFO - The valid find values : \n",
      "2018-10-12 21:19:54,195 - INFO - ['0.0', '1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0', '8.0', '9.0', '10.0']\n",
      "2018-10-12 21:19:54,196 - INFO - Finding whether find analysis is possible\n",
      "2018-10-12 21:20:06,402 - INFO - The index at which values need to be changed : \n",
      "2018-10-12 21:20:06,403 - DEBUG - []\n",
      "2018-10-12 21:20:06,404 - INFO - The number of violations : 0\n",
      "2018-10-12 21:20:06,405 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:20:06,406 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:20:06,407 - INFO - The find analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:20:06,408 - INFO - the number of valid values : 3252531\n",
      "2018-10-12 21:20:06,410 - INFO - The browser data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:20:06,411 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:20:06,412 - INFO - []\n",
      "2018-10-12 21:20:06,414 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:20:06,415 - INFO - The (find)cleaned data \n",
      "2018-10-12 21:20:06,416 - INFO - 2017-02-01\n",
      "2018-10-12 21:20:06,450 - INFO - checking on : crawler\n",
      "2018-10-12 21:20:06,451 - INFO - Checking and cleaning thecrawler\n",
      "2018-10-12 21:20:06,452 - INFO - cleaning the column : crawler\n",
      "2018-10-12 21:20:06,453 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:20:06,456 - INFO - The valid values : \n",
      "2018-10-12 21:20:06,457 - INFO - ['0.0', '1.0']\n",
      "2018-10-12 21:20:06,457 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:20:08,087 - DEBUG - The index at which values need to be changed : \n",
      "2018-10-12 21:20:08,088 - DEBUG - []\n",
      "2018-10-12 21:20:08,088 - INFO - The number of violations : 0\n",
      "2018-10-12 21:20:08,089 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:20:08,090 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:20:08,095 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:20:08,097 - INFO - the number of valid values : 3252531\n",
      "2018-10-12 21:20:08,100 - INFO - The crawler data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:20:08,102 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:20:08,104 - INFO - []\n",
      "2018-10-12 21:20:08,112 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:20:08,113 - INFO - checking on : idx\n",
      "2018-10-12 21:20:08,115 - INFO - Checking and cleaning theidx\n",
      "2018-10-12 21:20:08,117 - INFO - cleaning the column : idx\n",
      "2018-10-12 21:20:08,118 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:20:08,120 - INFO - The valid values : \n",
      "2018-10-12 21:20:08,122 - INFO - ['0.0', '1.0']\n",
      "2018-10-12 21:20:08,123 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:20:10,464 - DEBUG - The index at which values need to be changed : \n",
      "2018-10-12 21:20:10,465 - DEBUG - []\n",
      "2018-10-12 21:20:10,466 - INFO - The number of violations : 0\n",
      "2018-10-12 21:20:10,467 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:20:10,468 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:20:10,469 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:20:10,469 - INFO - the number of valid values : 3252531\n",
      "2018-10-12 21:20:10,476 - INFO - The idx data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:20:10,478 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:20:10,480 - INFO - []\n",
      "2018-10-12 21:20:10,482 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:20:10,484 - INFO - checking on : norefer\n",
      "2018-10-12 21:20:10,486 - INFO - Checking and cleaning thenorefer\n",
      "2018-10-12 21:20:10,489 - INFO - cleaning the column : norefer\n",
      "2018-10-12 21:20:10,490 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:20:10,492 - INFO - The valid values : \n",
      "2018-10-12 21:20:10,494 - INFO - ['0.0', '1.0']\n",
      "2018-10-12 21:20:10,495 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:20:12,090 - DEBUG - The index at which values need to be changed : \n",
      "2018-10-12 21:20:12,091 - DEBUG - []\n",
      "2018-10-12 21:20:12,092 - INFO - The number of violations : 0\n",
      "2018-10-12 21:20:12,092 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:20:12,093 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:20:12,094 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:20:12,099 - INFO - the number of valid values : 3252531\n",
      "2018-10-12 21:20:12,101 - INFO - The norefer data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:20:12,104 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:20:12,106 - INFO - []\n",
      "2018-10-12 21:20:12,108 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 21:20:12,110 - INFO - checking on : noagent\n",
      "2018-10-12 21:20:12,112 - INFO - Checking and cleaning thenoagent\n",
      "2018-10-12 21:20:12,114 - INFO - cleaning the column : noagent\n",
      "2018-10-12 21:20:12,116 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:20:12,118 - INFO - The valid values : \n",
      "2018-10-12 21:20:12,121 - INFO - ['0.0', '1.0']\n",
      "2018-10-12 21:20:12,123 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:20:13,701 - DEBUG - The index at which values need to be changed : \n",
      "2018-10-12 21:20:13,702 - DEBUG - []\n",
      "2018-10-12 21:20:13,703 - INFO - The number of violations : 0\n",
      "2018-10-12 21:20:13,704 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:20:13,705 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:20:13,705 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:20:13,706 - INFO - the number of valid values : 3252531\n",
      "2018-10-12 21:20:13,708 - INFO - The noagent data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:20:13,710 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:20:13,717 - INFO - []\n",
      "2018-10-12 21:20:13,718 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:20:13,720 - INFO - The float values cleaned data \n",
      "2018-10-12 21:20:13,721 - INFO - 2017-02-01\n",
      "2018-10-12 21:20:13,769 - INFO - [0. 1.]\n",
      "2018-10-12 21:20:13,773 - INFO - The float values cleaned data \n",
      "2018-10-12 21:20:13,774 - INFO - 2017-02-01\n",
      "2018-10-12 21:20:13,813 - INFO - [1. 0.]\n",
      "2018-10-12 21:20:13,814 - INFO - The float values cleaned data \n",
      "2018-10-12 21:20:13,815 - INFO - 2017-02-01\n",
      "2018-10-12 21:20:13,851 - INFO - [0.]\n",
      "2018-10-12 21:20:13,852 - INFO - The float values cleaned data \n",
      "2018-10-12 21:20:13,854 - INFO - 2017-02-01\n",
      "2018-10-12 21:20:13,887 - INFO - [0. 1.]\n",
      "2018-10-12 21:20:13,889 - INFO - checking on : size\n",
      "2018-10-12 21:20:13,890 - INFO - Checking and cleaning thesize\n",
      "2018-10-12 21:20:13,891 - INFO - cleaning the column : size\n",
      "2018-10-12 21:20:13,892 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:20:13,893 - INFO - The valid values : any valid number of type float\n",
      "2018-10-12 21:20:13,894 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:20:14,714 - INFO - The index at which values need to be changed : \n",
      "2018-10-12 21:20:14,715 - INFO - []\n",
      "2018-10-12 21:20:14,716 - INFO - The number of violations : 0\n",
      "2018-10-12 21:20:14,717 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:20:14,718 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:20:14,719 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:20:14,720 - INFO - the number of valid values : 3252531\n",
      "2018-10-12 21:20:14,722 - INFO - The size data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:20:14,723 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:20:14,725 - DEBUG - []\n",
      "2018-10-12 21:20:14,726 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:20:14,727 - INFO - checking on : zone\n",
      "2018-10-12 21:20:14,729 - INFO - Checking and cleaning thezone\n",
      "2018-10-12 21:20:14,730 - INFO - cleaning the column : zone\n",
      "2018-10-12 21:20:14,731 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:20:14,732 - INFO - The valid values : any valid number of type float\n",
      "2018-10-12 21:20:14,734 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:20:15,530 - INFO - The index at which values need to be changed : \n",
      "2018-10-12 21:20:15,531 - INFO - []\n",
      "2018-10-12 21:20:15,531 - INFO - The number of violations : 0\n",
      "2018-10-12 21:20:15,532 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:20:15,533 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:20:15,534 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:20:15,535 - INFO - the number of valid values : 3252531\n",
      "2018-10-12 21:20:15,537 - INFO - The zone data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:20:15,538 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:20:15,539 - DEBUG - []\n",
      "2018-10-12 21:20:15,541 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:20:15,542 - INFO - checking on : cik\n",
      "2018-10-12 21:20:15,543 - INFO - Checking and cleaning thecik\n",
      "2018-10-12 21:20:15,544 - INFO - cleaning the column : cik\n",
      "2018-10-12 21:20:15,546 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:20:15,547 - INFO - The valid values : any valid number of type float\n",
      "2018-10-12 21:20:15,563 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:20:16,584 - INFO - The index at which values need to be changed : \n",
      "2018-10-12 21:20:16,585 - INFO - []\n",
      "2018-10-12 21:20:16,586 - INFO - The number of violations : 0\n",
      "2018-10-12 21:20:16,587 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:20:16,588 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:20:16,589 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:20:16,591 - INFO - the number of valid values : 3252531\n",
      "2018-10-12 21:20:16,592 - INFO - The cik data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:20:16,594 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:20:16,595 - DEBUG - []\n",
      "2018-10-12 21:20:16,596 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:20:16,597 - INFO - The float values cleaned data \n",
      "2018-10-12 21:20:16,599 - INFO - 2017-02-01\n",
      "2018-10-12 21:20:16,761 - INFO - float64\n",
      "2018-10-12 21:20:16,763 - INFO - The float values cleaned data \n",
      "2018-10-12 21:20:16,764 - INFO - 2017-02-01\n",
      "2018-10-12 21:20:16,817 - INFO - float64\n",
      "2018-10-12 21:20:16,822 - INFO - The float values cleaned data \n",
      "2018-10-12 21:20:16,827 - INFO - 2017-02-01\n",
      "2018-10-12 21:20:16,928 - INFO - float64\n",
      "2018-10-12 21:20:16,929 - INFO - Checking and cleaning the code\n",
      "2018-10-12 21:20:16,930 - INFO - cleaning the column : code\n",
      "2018-10-12 21:20:16,931 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:20:16,932 - INFO - The valid values : 1xx 2xx 3xx 4xx 5xx\n",
      "2018-10-12 21:20:16,933 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:20:21,343 - INFO - The index at which values need to be changed : \n",
      "2018-10-12 21:20:21,344 - INFO - []\n",
      "2018-10-12 21:20:21,345 - INFO - The number of violations : 8399\n",
      "2018-10-12 21:20:21,346 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:20:21,347 - INFO - Total number of valid entries : 3244132\n",
      "2018-10-12 21:20:21,347 - INFO - The analysis is possible as the value is : 99.74177033208908\n",
      "2018-10-12 21:20:21,348 - INFO - the number of valid values : 3244132\n",
      "2018-10-12 21:20:21,350 - INFO - The browser data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:20:21,351 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:20:21,353 - INFO - []\n",
      "2018-10-12 21:20:21,355 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:20:21,356 - INFO - The (code)cleaned data \n",
      "2018-10-12 21:20:21,358 - INFO - 2017-02-01\n",
      "2018-10-12 21:20:21,413 - INFO - Checking and cleaning the date\n",
      "2018-10-12 21:20:21,414 - INFO - cleaning the column : date\n",
      "2018-10-12 21:20:21,416 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:20:21,417 - INFO - The valid values : year-month-day\n",
      "2018-10-12 21:20:21,418 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:20:27,345 - INFO - The index at which values need to be changed : \n",
      "2018-10-12 21:20:27,346 - INFO - []\n",
      "2018-10-12 21:20:27,347 - INFO - The number of violations : 0\n",
      "2018-10-12 21:20:27,348 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:20:27,349 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:20:27,350 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:20:27,359 - INFO - the number of valid values : 3252531\n",
      "2018-10-12 21:20:27,361 - INFO - The  data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:20:27,364 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:20:27,366 - INFO - []\n",
      "2018-10-12 21:20:27,370 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:20:27,373 - INFO - The (date)cleaned data \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 21:20:27,377 - INFO - 2017-02-01\n",
      "2018-10-12 21:20:27,570 - DEBUG - ['2017-02-01']\n",
      "2018-10-12 21:20:27,571 - INFO - Checking and cleaning the time\n",
      "2018-10-12 21:20:27,572 - INFO - cleaning the column : time\n",
      "2018-10-12 21:20:27,573 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:20:27,573 - INFO - The valid values : %H:%M:%S\n",
      "2018-10-12 21:20:27,575 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:21:08,466 - INFO - The index at which values need to be changed : \n",
      "2018-10-12 21:21:08,467 - INFO - []\n",
      "2018-10-12 21:21:08,468 - INFO - The number of violations : 0\n",
      "2018-10-12 21:21:08,469 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:21:08,469 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:21:08,470 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:21:08,471 - INFO - the number of valid values : 3252531\n",
      "2018-10-12 21:21:08,472 - INFO - The  data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:21:08,473 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:21:08,474 - INFO - []\n",
      "2018-10-12 21:21:08,476 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:21:08,477 - INFO - The (time)cleaned data \n",
      "2018-10-12 21:21:08,479 - INFO - 2017-02-01\n",
      "2018-10-12 21:21:08,481 - INFO - Checking and cleaning the ip\n",
      "2018-10-12 21:21:08,482 - INFO - cleaning the column : ip\n",
      "2018-10-12 21:21:08,482 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:21:08,484 - INFO - The valid values : ###.###.###.xxx\n",
      "2018-10-12 21:21:08,484 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:21:11,870 - INFO - The index at which values need to be changed : \n",
      "2018-10-12 21:21:11,871 - INFO - []\n",
      "2018-10-12 21:21:11,872 - INFO - The number of violations : 0\n",
      "2018-10-12 21:21:11,873 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:21:11,873 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:21:11,874 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:21:11,875 - INFO - the number of valid values : 3252531\n",
      "2018-10-12 21:21:11,876 - INFO - The  data is sufficient for analysis : 2017-02-01\n",
      "2018-10-12 21:21:11,877 - INFO - index of find where wrong values are present :\n",
      "2018-10-12 21:21:11,879 - DEBUG - []\n",
      "2018-10-12 21:21:11,880 - INFO - Cleaning of data not required as it doesnt contain wrong values : 2017-02-01\n",
      "2018-10-12 21:21:11,881 - INFO - The (ip)cleaned data \n",
      "2018-10-12 21:21:11,882 - INFO - 2017-02-01\n",
      "2018-10-12 21:21:11,883 - INFO - checking on : cik\n",
      "2018-10-12 21:21:11,884 - INFO - cleaning the column : cik\n",
      "2018-10-12 21:21:11,885 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:21:11,886 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:21:11,946 - INFO - The number of violations : 0\n",
      "2018-10-12 21:21:11,947 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:21:11,948 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:21:11,949 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:21:11,950 - INFO - analysis possible with : cik\n",
      "2018-10-12 21:21:11,951 - INFO - checking on : accession\n",
      "2018-10-12 21:21:11,952 - INFO - cleaning the column : accession\n",
      "2018-10-12 21:21:11,953 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:21:11,954 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:21:12,108 - INFO - The number of violations : 0\n",
      "2018-10-12 21:21:12,109 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:21:12,110 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:21:12,111 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:21:12,112 - INFO - analysis possible with : accession\n",
      "2018-10-12 21:21:12,113 - INFO - checking on : extention\n",
      "2018-10-12 21:21:12,114 - INFO - cleaning the column : extention\n",
      "2018-10-12 21:21:12,115 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:21:12,116 - INFO - Finding whether analysis is possible\n",
      "2018-10-12 21:21:12,248 - INFO - The number of violations : 0\n",
      "2018-10-12 21:21:12,249 - INFO - The total rows : 3252531\n",
      "2018-10-12 21:21:12,250 - INFO - Total number of valid entries : 3252531\n",
      "2018-10-12 21:21:12,251 - INFO - The analysis is possible as the value is : 100.0\n",
      "2018-10-12 21:21:12,251 - INFO - analysis possible with : extention\n",
      "2018-10-12 21:21:12,252 - INFO - checking whether all columns have same number of rows\n",
      "2018-10-12 21:21:12,253 - INFO - The total entries in the dataframe : 3252531\n",
      "2018-10-12 21:21:12,255 - INFO - The valid columns['ip', 'date', 'time', 'zone', 'cik', 'accession', 'extention', 'code', 'size', 'idx', 'norefer', 'noagent', 'find', 'crawler', 'browser']\n",
      "2018-10-12 21:21:12,256 - INFO - No data missing in : ip\n",
      "2018-10-12 21:21:12,256 - INFO - No data missing in : date\n",
      "2018-10-12 21:21:12,257 - INFO - No data missing in : time\n",
      "2018-10-12 21:21:12,258 - INFO - No data missing in : zone\n",
      "2018-10-12 21:21:12,259 - INFO - No data missing in : cik\n",
      "2018-10-12 21:21:12,260 - INFO - No data missing in : accession\n",
      "2018-10-12 21:21:12,261 - INFO - No data missing in : extention\n",
      "2018-10-12 21:21:12,263 - INFO - No data missing in : code\n",
      "2018-10-12 21:21:12,264 - INFO - No data missing in : size\n",
      "2018-10-12 21:21:12,264 - INFO - No data missing in : idx\n",
      "2018-10-12 21:21:12,266 - INFO - No data missing in : norefer\n",
      "2018-10-12 21:21:12,266 - INFO - No data missing in : noagent\n",
      "2018-10-12 21:21:12,267 - INFO - No data missing in : find\n",
      "2018-10-12 21:21:12,269 - INFO - No data missing in : crawler\n",
      "2018-10-12 21:21:12,270 - INFO - No data missing in : browser\n",
      "2018-10-12 21:21:12,271 - INFO - There are no missing data\n",
      "2018-10-12 21:21:12,272 - INFO - Data clean is complete\n",
      "2018-10-12 21:21:12,273 - INFO - ######################\n",
      "2018-10-12 21:21:12,274 - INFO - After data clean\n",
      "2018-10-12 21:21:12,275 - INFO - ######################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Desktop\\ADS\\INFO7390-Group11\\Part2/201720181012_210517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12 21:21:56,513 - INFO - ######################\n",
      "2018-10-12 21:21:56,514 - INFO - Converting each dataframe to csv\n",
      "2018-10-12 21:21:56,515 - INFO - ######################\n",
      "2018-10-12 21:21:56,516 - INFO - ######################\n",
      "2018-10-12 21:21:56,517 - INFO - Summary Analysis\n",
      "2018-10-12 21:21:56,518 - INFO - ######################\n",
      "2018-10-12 21:21:56,519 - INFO - Starting the summary analysis on the cleaned data\n",
      "2018-10-12 21:21:56,520 - INFO - Request Response code analysis\n",
      "2018-10-12 21:21:56,535 - INFO - 200 OK percentage : Analysis\n",
      "2018-10-12 21:21:56,542 - INFO - Total number of entries in 3252531\n",
      "2018-10-12 21:21:56,626 - INFO - The stats of response codes on : 2017-02-01\n",
      "2018-10-12 21:21:56,635 - INFO - 200.0    2401226\n",
      "301.0     706803\n",
      "304.0      77183\n",
      "206.0      27991\n",
      "503.0      17311\n",
      "0.0         8399\n",
      "403.0       5717\n",
      "404.0       4235\n",
      "429.0       3590\n",
      "500.0         66\n",
      "504.0          9\n",
      "502.0          1\n",
      "Name: code, dtype: int64\n",
      "2018-10-12 21:21:56,647 - INFO - The total number of 200 : 2401226\n",
      "2018-10-12 21:21:56,656 - INFO - percentage73.82638320741601%\n",
      "2018-10-12 21:21:56,666 - INFO - 404 Not found percentage : Analysis\n",
      "2018-10-12 21:21:56,676 - INFO - Total number of entries in 3252531\n",
      "2018-10-12 21:21:56,766 - INFO - The stats of response codes on : 2017-02-01\n",
      "2018-10-12 21:21:56,774 - INFO - 200.0    2401226\n",
      "301.0     706803\n",
      "304.0      77183\n",
      "206.0      27991\n",
      "503.0      17311\n",
      "0.0         8399\n",
      "403.0       5717\n",
      "404.0       4235\n",
      "429.0       3590\n",
      "500.0         66\n",
      "504.0          9\n",
      "502.0          1\n",
      "Name: code, dtype: int64\n",
      "2018-10-12 21:21:56,786 - INFO - The total number of 404 : 4235\n",
      "2018-10-12 21:21:56,797 - INFO - percentage0.13020629165409953%\n",
      "2018-10-12 21:21:56,805 - INFO - To find the bussiest hour of the day : 2017-02-01\n",
      "2018-10-12 21:21:57,107 - INFO - The summary of number of requests at all hours\n",
      "2018-10-12 21:21:57,115 - INFO - {1: 1138725, 2: 1286917}\n",
      "2018-10-12 21:21:57,123 - INFO - The bussiest hour : 2 and Number of requests : 1286917\n",
      "2018-10-12 21:21:57,130 - INFO - percentage of requests : 39.56663287759594%\n",
      "2018-10-12 21:21:57,140 - INFO - Average number of requests per hour : 1212821\n",
      "2018-10-12 21:21:57,147 - INFO - Average percentage : 37%\n",
      "2018-10-12 21:21:57,156 - INFO - To find the top 5 IP addresses making maximum number of requets in the day : 2017-02-01\n",
      "2018-10-12 21:21:57,164 - INFO - grouping data by IP\n",
      "2018-10-12 21:21:57,560 - INFO - sorting in descending order\n",
      "2018-10-12 21:21:57,564 - INFO - TOP 5 IPs\n",
      "2018-10-12 21:21:57,572 - INFO -                     ip     cik\n",
      "2452   130.101.154.hhj  280781\n",
      "10393    54.152.17.ccg  142652\n",
      "13359   72.234.116.hbh  103781\n",
      "764      107.23.85.jfd   79238\n",
      "9922      50.16.61.jgj   72966\n",
      "2018-10-12 21:21:57,587 - INFO - percentage of requests from top 1 IP :130.101.154.hhj\n",
      "2018-10-12 21:21:57,597 - INFO - 8.632692509310441\n",
      "2018-10-12 21:21:57,742 - INFO - Total data size accessed on the date : 2017-02-01\n",
      "2018-10-12 21:21:57,751 - INFO - 244196261718.0\n",
      "2018-10-12 21:21:57,759 - INFO - successful data size accessed on the date : 2017-02-01\n",
      "2018-10-12 21:21:57,767 - INFO - 232236117870.0\n",
      "2018-10-12 21:21:57,778 - INFO - percentage success95.10224122029695%\n",
      "2018-10-12 21:21:57,791 - INFO - Average size of the dat being accessed : 75078\n",
      "2018-10-12 21:21:57,805 - INFO - To find the top 10 page type requeted in the day : 2017-02-01\n",
      "2018-10-12 21:21:57,816 - INFO - grouping data by extention\n",
      "2018-10-12 21:21:58,514 - INFO - sorting in descending order\n",
      "2018-10-12 21:21:58,544 - INFO - TOP 10 extentions\n",
      "2018-10-12 21:21:58,552 - DEBUG -                           extention      cik\n",
      "58                       -index.htm  2048270\n",
      "68                             .txt   540710\n",
      "114901              primary_doc.xml    31014\n",
      "60                      -index.html    26161\n",
      "70303                      doc4.xml    20873\n",
      "74251                     edgar.xml    14981\n",
      "153747  xslFormDX01/primary_doc.xml    13986\n",
      "149208          xslF345X03/doc4.xml    12320\n",
      "87711                     form4.xml    10652\n",
      "149286         xslF345X03/edgar.xml     6814\n",
      "2018-10-12 21:21:58,574 - INFO - Number of requests with the extention asked frequesntly:-index.htm\n",
      "2018-10-12 21:21:58,583 - INFO - Percent : \n",
      "2018-10-12 21:21:58,593 - INFO - 62.97464958827449\n",
      "2018-10-12 21:21:58,604 - INFO - Summary analysis completed\n",
      "2018-10-12 21:21:58,612 - INFO - ######################\n",
      "2018-10-12 21:21:58,613 - INFO - Zipping the file with CSVs and making a zip folder\n",
      "2018-10-12 21:21:58,614 - INFO - ######################\n",
      "2018-10-12 21:22:15,240 - INFO - ######################\n",
      "2018-10-12 21:22:15,241 - INFO - Pushing the files to S3\n",
      "2018-10-12 21:22:15,241 - INFO - ######################\n",
      "2018-10-12 21:22:15,243 - INFO - ######################\n",
      "2018-10-12 21:22:15,243 - INFO - DONE\n",
      "2018-10-12 21:22:15,244 - INFO - ######################\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-2029074f5a5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_dataframes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"######################\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-ec4413377204>\u001b[0m in \u001b[0;36mcreate_dataframes\u001b[1;34m(i, path)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#    return pd.read_csv(list_val)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "\n",
    "for index in range(0,1):\n",
    "    \n",
    "    path = str(os.getcwd())+'/'+year\n",
    "    \n",
    "    df=create_dataframes(index,path)\n",
    "\n",
    "    logger.info(\"######################\")\n",
    "    logger.info(\"The dataframe created successfully\")\n",
    "    logger.info(\"######################\")\n",
    "\n",
    "    logger.info(\"######################\")\n",
    "    logger.info(\"printing the dataframes\")\n",
    "    logger.info(\"######################\")\n",
    "    logger.info(df.head(5))\n",
    "\n",
    "    logger.info(\"######################\")\n",
    "    logger.info(\"Cleaning the data\")\n",
    "    logger.info(\"######################\")\n",
    "\n",
    "    df = dataclean(df)\n",
    "\n",
    "    \n",
    "    logger.info(\"######################\")\n",
    "    logger.info(\"After data clean\")\n",
    "    logger.info(\"######################\")\n",
    "\n",
    "    \n",
    "    convert_dataframes_to_csv(df)\n",
    "    \n",
    "    logger.info(\"######################\")\n",
    "    logger.info(\"Converting each dataframe to csv\")\n",
    "    logger.info(\"######################\")\n",
    "\n",
    "    logger.info(\"######################\")\n",
    "    logger.info(\"Summary Analysis\")\n",
    "    logger.info(\"######################\")\n",
    "\n",
    "    summaryanalysis(df)\n",
    "    \n",
    "    \n",
    "logger.info(\"######################\")\n",
    "logger.info(\"Zipping the file with CSVs and making a zip folder\")\n",
    "logger.info(\"######################\")\n",
    "\n",
    "zipdirmain()\n",
    "    \n",
    "logger.info(\"######################\")\n",
    "logger.info(\"Pushing the files to S3\")\n",
    "logger.info(\"######################\")\n",
    "\n",
    "logger.info(\"######################\")\n",
    "logger.info(\"DONE\")\n",
    "logger.info(\"######################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
